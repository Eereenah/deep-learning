{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Deoldify-GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eereenah/deep-learning/blob/master/Deoldify_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3i_2Yl6G-Ok",
        "colab_type": "text"
      },
      "source": [
        "# DS-330 Final Project: Deoldify\n",
        "\n",
        "The conversion of a colored image into gray-scale is very simple. However, the inverse operation is not so trivial, hence, automatic image colorization has been a popular area of research in Deep Learning for quite a while. As the information in the black-and-white domain is relatively limited, the addition of color aspect can provide the image with new semantic meanings. \n",
        "\n",
        "The following notebook features the modified implementation of pix2pix network suitable for the colorization task.\n",
        "![example](https://i.imgur.com/4oB9X0c.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDC9LzSQZSKn",
        "colab_type": "text"
      },
      "source": [
        "## Imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CARprE8gdLX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# from viz import updatable_display2\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms, datasets\n",
        "import torchvision.utils as vutils\n",
        "from skimage import color\n",
        "\n",
        "# use_cuda = True\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf46nH_4dqAL",
        "colab_type": "text"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAkcKXJFZaY5",
        "colab_type": "text"
      },
      "source": [
        "Because of limited computational resources, we will be using a subset of COCO dataset (the `test2017` subset, which features 40K images for training and `val2017`, 5K images for testing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jEOdXuCdrd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget images.cocodataset.org/zips/val2017.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShMwKz70d5bV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget images.cocodataset.org/zips/test2017.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNPn7mJkd7Tq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! unzip -qq *.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Niuvkta1d3S7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm *.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilh-xcLGT6_b",
        "colab_type": "text"
      },
      "source": [
        "Images for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI28Jirfet2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! find 'test2017/' -maxdepth 1 -type f -printf \".\" | wc -c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muc3dvN0T8xT",
        "colab_type": "text"
      },
      "source": [
        "Images for testing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47Jlh8iUT4bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! find 'val2017/' -maxdepth 1 -type f -printf \".\" | wc -c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ungIvdtU8-wt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPgKFT5cdLYC",
        "colab_type": "text"
      },
      "source": [
        "## Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khy2Y2t0Z-FW",
        "colab_type": "text"
      },
      "source": [
        "Helper fucntions for reading and resizing the images:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVvcuWzRdLY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "def getPaths(root):\n",
        "    for file in os.listdir(root):\n",
        "        if 'jpg' in file:\n",
        "            yield(os.path.join(root,file))\n",
        "\n",
        "def readimg(l):\n",
        "    im = cv2.imread(l)\n",
        "    return cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def resize(img, size):\n",
        "    img = np.array(img, dtype=np.uint8)\n",
        "    if len(img.shape) == 2:\n",
        "      img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "    return cv2.resize(img, (size, size)) \n",
        "\n",
        "def black_and_white(img):\n",
        "    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmYG-M5NwXmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root = 'test2017/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRIHvcWnaIK7",
        "colab_type": "text"
      },
      "source": [
        "Creating a custom data-loader, to create bw-color image pairs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcTGnOslw1k1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Load_Dataset(Dataset):\n",
        "    \"\"\"Create Dataloaders\"\"\"\n",
        "    def __init__(self, root_dir, size, transform = None):\n",
        "\n",
        "        self.paths = list(getPaths(root_dir))\n",
        "        self.size = size\n",
        "        self.img_transform = transforms.Compose([\n",
        "                   transforms.ToTensor(),\n",
        "                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "               ])\n",
        "        self.bw_transform = transforms.Compose([\n",
        "                   transforms.ToTensor(),\n",
        "               ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = resize(readimg(self.paths[idx]), self.size)\n",
        "        return self.img_transform(img), self.bw_transform(black_and_white(img)[:,:,None])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdsG0j8FdLZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = Load_Dataset(root,128)\n",
        "dataset[0][0].shape,dataset[0][1].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKY96eCMdLZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset[0][0].min(), dataset[0][0].max()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soww_SnXdLZU",
        "colab_type": "text"
      },
      "source": [
        "## Model\n",
        "\n",
        "The pix2pix model uses\n",
        "\n",
        "* a U-Net model as a generator $G$. This model takes an image $y$ as input and produces another image $x$ \n",
        "* a convolutional descriminator model $D$. This model takes both $x$ and $y$ as input and tries to guess if $x \\sim p_x$ or $x \\sim p_G$\n",
        "\n",
        "The model is trained using the GAN loss function and an additional $L_1$ loss on the output of the generative model $G$. Both losses are balanced using an additional hyperparameter $\\lambda$ (set to 100 in the original paper). \n",
        "Additionally, for $L_{GAN}$, we use the gradient penalty to ensure the Lipschitz property (WGAN-GP). The final objective function is defined as:\n",
        "\n",
        "$$ G^*  = \\arg\\min_G\\max_D \\mathcal{L}_{cGAN}(G,D)_{WP} + \\lambda \\mathcal{L}_{L1}(G) $$ where $\\mathcal{L}_{cGAN}(G,D)_{WP}$ is defined as the sum of conditional GAN loss and WGAN Gradient Penalty\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31NgmaMLa-2n",
        "colab_type": "text"
      },
      "source": [
        "As for the network architecture, we adapt that of pix2pix. In particular, the generator borrows from the U-Net architecture with skip-connections, which allow for sharing of low-level information on the input and output levels such as the location of edges, which provide crucial information for the colorization task. We use snippets from the Pytorch implementation of the generator/discriminator in our project:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uwb4dQkdLZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class gated_resnet(nn.Module):\n",
        "    \"\"\"\n",
        "    Gated Residual Block\n",
        "    \"\"\"\n",
        "    def __init__(self, num_filters, kernel_size, padding, nonlinearity=nn.ReLU, dropout=0.2, dilation=1,batchNormObject=nn.BatchNorm2d):\n",
        "        super(gated_resnet, self).__init__()\n",
        "        self.gated = True\n",
        "        num_hidden_filters =2 * num_filters if gated else num_filters\n",
        "        self.conv_input = nn.Conv2d(num_filters, num_hidden_filters, kernel_size=kernel_size,stride=1,padding=padding,dilation=dilation )\n",
        "        self.dropout = nn.Dropout2d(dropout)\n",
        "        self.nonlinearity = nonlinearity()\n",
        "        self.batch_norm1 = batchNormObject(num_hidden_filters)\n",
        "        self.conv_out = nn.Conv2d(num_hidden_filters, num_hidden_filters, kernel_size=kernel_size,stride=1,padding=padding,dilation=dilation )\n",
        "        self.batch_norm2 = batchNormObject(num_filters)\n",
        "\n",
        "    def forward(self, og_x):\n",
        "        x = self.conv_input(og_x)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.nonlinearity(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv_out(x)\n",
        "        if self.gated:\n",
        "            a, b = torch.chunk(x, 2, dim=1)\n",
        "            c3 = a * F.sigmoid(b)\n",
        "        else:\n",
        "            c3 = x\n",
        "        out = og_x + c3\n",
        "        out = self.batch_norm2(out)\n",
        "        return out\n",
        "    \n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual Block\n",
        "    \"\"\"\n",
        "    def __init__(self, num_filters, kernel_size, padding, nonlinearity=nn.ReLU, dropout=0.2, dilation=1,batchNormObject=nn.BatchNorm2d):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        num_hidden_filters = num_filters\n",
        "        self.conv1 = nn.Conv2d(num_filters, num_hidden_filters, kernel_size=kernel_size,stride=1,padding=padding,dilation=dilation )\n",
        "        self.dropout = nn.Dropout2d(dropout)\n",
        "        self.nonlinearity = nonlinearity(inplace=False)\n",
        "        self.batch_norm1 = batchNormObject(num_hidden_filters)\n",
        "        self.conv2 = nn.Conv2d(num_hidden_filters, num_hidden_filters, kernel_size=kernel_size,stride=1,padding=padding,dilation=dilation )\n",
        "        self.batch_norm2 = batchNormObject(num_filters)\n",
        "\n",
        "    def forward(self, og_x):\n",
        "        x = og_x\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv1(og_x)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.nonlinearity(x)\n",
        "        x = self.conv2(x)\n",
        "        out = og_x + x\n",
        "        out = self.batch_norm2(out)\n",
        "        out = self.nonlinearity(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFPnfqpfkFER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvolutionalEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional Encoder providing skip connections\n",
        "    \"\"\"\n",
        "    def __init__(self,n_features_input,num_hidden_features,kernel_size,padding,n_resblocks,dropout_min=0,dropout_max=0.2, blockObject=ResidualBlock,batchNormObject=nn.BatchNorm2d):\n",
        "        \"\"\"\n",
        "        n_features_input (int): number of intput features\n",
        "        num_hidden_features (list(int)): number of features for each stage\n",
        "        kernel_size (int): convolution kernel size\n",
        "        padding (int): convolution padding\n",
        "        n_resblocks (int): number of residual blocks at each stage\n",
        "        dropout (float): dropout probability\n",
        "        blockObject (nn.Module): Residual block to use. Default is ResidualBlock\n",
        "        batchNormObject (nn.Module): normalization layer. Default is nn.BatchNorm2d\n",
        "        \"\"\"\n",
        "        super(ConvolutionalEncoder,self).__init__()\n",
        "        self.n_features_input = n_features_input\n",
        "        self.num_hidden_features = num_hidden_features\n",
        "        self.stages = nn.ModuleList()\n",
        "        dropout = iter([(1-t)*dropout_min + t*dropout_max   for t in np.linspace(0,1,(len(num_hidden_features)))])\n",
        "        dropout = iter(dropout)\n",
        "        # input convolution block\n",
        "        block = [nn.Conv2d(n_features_input, num_hidden_features[0], kernel_size=kernel_size,stride=1, padding=padding)]\n",
        "        for _ in range(n_resblocks):\n",
        "            p = next(iter(dropout))\n",
        "            block += [blockObject(num_hidden_features[0], kernel_size, padding, dropout=p,batchNormObject=batchNormObject)]\n",
        "        self.stages.append(nn.Sequential(*block))\n",
        "        # layers\n",
        "        for features_in,features_out in [num_hidden_features[i:i+2] for i in range(0,len(num_hidden_features), 1)][:-1]:\n",
        "            # downsampling\n",
        "            block = [nn.MaxPool2d(2),nn.Conv2d(features_in, features_out, kernel_size=1,padding=0 ),batchNormObject(features_out),nn.ReLU()]\n",
        "            #block = [nn.Conv2d(features_in, features_out, kernel_size=kernel_size,stride=2,padding=padding ),nn.BatchNorm2d(features_out),nn.ReLU()]\n",
        "            # residual blocks\n",
        "            p = next(iter(dropout))\n",
        "            for _ in range(n_resblocks):\n",
        "                block += [blockObject(features_out, kernel_size, padding, dropout=p,batchNormObject=batchNormObject)]\n",
        "            self.stages.append(nn.Sequential(*block)) \n",
        "            \n",
        "    def forward(self,x):\n",
        "        skips = []\n",
        "        for stage in self.stages:\n",
        "            x = stage(x)\n",
        "            skips.append(x)\n",
        "        return x,skips\n",
        "    def getInputShape(self):\n",
        "        return (-1,self.n_features_input,-1,-1)\n",
        "    def getOutputShape(self):\n",
        "        return (-1,self.num_hidden_features[-1], -1,-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLpDLGSqkJoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvolutionalDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional Decoder taking skip connections\n",
        "    \"\"\"\n",
        "    def __init__(self,n_features_output,num_hidden_features,kernel_size,padding,n_resblocks,dropout_min=0,dropout_max=0.2,blockObject=ResidualBlock,batchNormObject=nn.BatchNorm2d):\n",
        "        \"\"\"\n",
        "        n_features_output (int): number of output features\n",
        "        num_hidden_features (list(int)): number of features for each stage\n",
        "        kernel_size (int): convolution kernel size\n",
        "        padding (int): convolution padding\n",
        "        n_resblocks (int): number of residual blocks at each stage\n",
        "        dropout (float): dropout probability\n",
        "        blockObject (nn.Module): Residual block to use. Default is ResidualBlock\n",
        "        batchNormObject (nn.Module): normalization layer. Default is nn.BatchNorm2d\n",
        "        \"\"\"\n",
        "        super(ConvolutionalDecoder,self).__init__()\n",
        "        self.n_features_output = n_features_output\n",
        "        self.num_hidden_features = num_hidden_features\n",
        "        self.upConvolutions = nn.ModuleList()\n",
        "        self.skipMergers = nn.ModuleList()\n",
        "        self.residualBlocks = nn.ModuleList()\n",
        "        dropout = iter([(1-t)*dropout_min + t*dropout_max   for t in np.linspace(0,1,(len(num_hidden_features)))][::-1])\n",
        "        # input convolution block\n",
        "        # layers\n",
        "        for features_in,features_out in [num_hidden_features[i:i+2] for i in range(0,len(num_hidden_features), 1)][:-1]:\n",
        "            # downsampling\n",
        "            self.upConvolutions.append(nn.Sequential(nn.ConvTranspose2d(features_in, features_out, kernel_size=3, stride=2,padding=1,output_padding=1),batchNormObject(features_out),nn.ReLU()))\n",
        "            self.skipMergers.append(nn.Conv2d(2*features_out, features_out, kernel_size=kernel_size,stride=1, padding=padding))\n",
        "            # residual blocks\n",
        "            block = []\n",
        "            p = next(iter(dropout))\n",
        "            for _ in range(n_resblocks):\n",
        "                block += [blockObject(features_out, kernel_size, padding, dropout=p,batchNormObject=batchNormObject)]\n",
        "            self.residualBlocks.append(nn.Sequential(*block))   \n",
        "        # output convolution block\n",
        "        block = [nn.Conv2d(num_hidden_features[-1],n_features_output, kernel_size=kernel_size,stride=1, padding=padding)]\n",
        "        self.output_convolution = nn.Sequential(*block)\n",
        "\n",
        "    def forward(self,x, skips):\n",
        "        for up,merge,conv,skip in zip(self.upConvolutions,self.skipMergers, self.residualBlocks,skips):\n",
        "            x = up(x)\n",
        "            cat = torch.cat([x,skip],1)\n",
        "            x = merge(cat)\n",
        "            x = conv(x)\n",
        "        return self.output_convolution(x)\n",
        "    def getInputShape(self):\n",
        "        return (-1,self.num_hidden_features[0],-1,-1)\n",
        "    def getOutputShape(self):\n",
        "        return (-1,self.n_features_output, -1,-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FfFg05tkO98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DilatedConvolutions(nn.Module):\n",
        "    \"\"\"\n",
        "    Sequential Dialted convolutions\n",
        "    \"\"\"\n",
        "    def __init__(self, n_channels, n_convolutions, dropout):\n",
        "        super(DilatedConvolutions, self).__init__()\n",
        "        kernel_size = 3\n",
        "        padding = 1\n",
        "        self.dropout = nn.Dropout2d(dropout)\n",
        "        self.non_linearity = nn.ReLU(inplace=True)\n",
        "        self.strides = [2**(k+1) for k in range(n_convolutions)]\n",
        "        convs = [nn.Conv2d(n_channels, n_channels, kernel_size=kernel_size,dilation=s, padding=s) for s in self.strides ]\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.bns = nn.ModuleList()\n",
        "        for c in convs:\n",
        "            self.convs.append(c)\n",
        "            self.bns.append(nn.BatchNorm2d(n_channels))\n",
        "    def forward(self,x):\n",
        "        skips = []\n",
        "        for (c,bn,s) in zip(self.convs,self.bns,self.strides):\n",
        "            x_in = x\n",
        "            x = c(x)\n",
        "            x = bn(x)\n",
        "            x = self.non_linearity(x)\n",
        "            x = self.dropout(x)\n",
        "            x = x_in + x\n",
        "            skips.append(x)\n",
        "        return x,skips"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-JYM3M0kRWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DilatedConvolutions2(nn.Module):\n",
        "    \"\"\"\n",
        "    Sequential Dialted convolutions\n",
        "    \"\"\"\n",
        "    def __init__(self, n_channels, n_convolutions,dropout,kernel_size,blockObject=ResidualBlock,batchNormObject=nn.BatchNorm2d):\n",
        "        super(DilatedConvolutions2, self).__init__()\n",
        "        self.dilatations = [2**(k+1) for k in range(n_convolutions)]\n",
        "        self.blocks = nn.ModuleList([blockObject(n_channels, kernel_size, d, dropout=dropout, dilation=d,batchNormObject=batchNormObject) for d in self.dilatations ])\n",
        "    def forward(self,x):\n",
        "        skips = []\n",
        "        for b in self.blocks:\n",
        "            x = b(x)\n",
        "            skips.append(x)\n",
        "        return x, skips"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMN4YLmBkT5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net model with dynamic number of layers, Residual Blocks, Dilated Convolutions, Dropout and Group Normalization\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, num_hidden_features,n_resblocks,num_dilated_convs, dropout_min=0, dropout_max=0, gated=False, padding=1, kernel_size=3,group_norm=32):\n",
        "        \"\"\"\n",
        "        initialize the model\n",
        "        Args:\n",
        "            in_channels (int): number of input channels (image=3)\n",
        "            out_channels (int): number of output channels (n_classes)\n",
        "            num_hidden_features (list(int)): number of hidden features for each layer (the number of layer is the lenght of this list)\n",
        "            n_resblocks (int): number of residual blocks at each layer \n",
        "            num_dilated_convs (int): number of dilated convolutions at the last layer\n",
        "            dropout (float): float in [0,1]: dropout probability\n",
        "            gated (bool): use gated Convolutions, default is False\n",
        "            padding (int): padding for the convolutions\n",
        "            kernel_size (int): kernel size for the convolutions\n",
        "            group_norm (bool): number of groups to use for Group Normalization, default is 32, if zero: use nn.BatchNorm2d\n",
        "        \"\"\"\n",
        "        super(UNet, self).__init__()\n",
        "        if group_norm > 0:\n",
        "            for h in num_hidden_features:\n",
        "                assert h%group_norm==0, \"Number of features at each layer must be divisible by 'group_norm'\"\n",
        "        blockObject = gated_resnet if gated else ResidualBlock\n",
        "        batchNormObject = lambda n_features : nn.GroupNorm(group_norm,n_features) if group_norm > 0 else nn.BatchNorm2d\n",
        "        self.encoder = ConvolutionalEncoder(in_channels,num_hidden_features,kernel_size,padding,n_resblocks,dropout_min=dropout_min,dropout_max=dropout_max,blockObject=blockObject,batchNormObject=batchNormObject)\n",
        "        if num_dilated_convs > 0:\n",
        "            #self.dilatedConvs = DilatedConvolutions2(num_hidden_features[-1], num_dilated_convs,dropout_max,kernel_size,blockObject=blockObject,batchNormObject=batchNormObject)\n",
        "            self.dilatedConvs = DilatedConvolutions(num_hidden_features[-1],num_dilated_convs,dropout_max) # <v11 uses dilatedConvs2\n",
        "        else:\n",
        "            self.dilatedConvs = None\n",
        "        self.decoder = ConvolutionalDecoder(out_channels,num_hidden_features[::-1],kernel_size,padding,n_resblocks,dropout_min=dropout_min,dropout_max=dropout_max,blockObject=blockObject,batchNormObject=batchNormObject)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x,skips = self.encoder(x)\n",
        "        if self.dilatedConvs is not None:\n",
        "            x,dilated_skips = self.dilatedConvs(x)\n",
        "            for d in dilated_skips:\n",
        "                x += d\n",
        "            x += skips[-1]\n",
        "        x = self.decoder(x,skips[:-1][::-1])\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBYspTVCbbkt",
        "colab_type": "text"
      },
      "source": [
        "Define the Generator $G$ and the dicrimonator $D$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yh2yItqkWSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, kwargs):\n",
        "        super(Generator, self).__init__()\n",
        "        self.unet = UNet(**kwargs)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.tanh(self.unet(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6sH65sonELy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, kwargs):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.encoder = ConvolutionalEncoder(**kwargs)\n",
        "        self.convout = nn.Conv2d(kwargs['num_hidden_features'][-1],1,kernel_size=3,padding=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        output,skips = self.encoder(input)\n",
        "        output = self.convout(output)\n",
        "        return output\n",
        "    \n",
        "class Identity(nn.Module):\n",
        "    def __init__(self,features):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COi7xyFXdLZo",
        "colab_type": "text"
      },
      "source": [
        "## Defining the model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPToN-FPbvFz",
        "colab_type": "text"
      },
      "source": [
        "Define hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eaAsVJ2dLZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "#m_test = int(np.sqrt(batch_size))\n",
        "m_test = 5\n",
        "m_test = m_test - 1 if m_test**2 > batch_size else m_test\n",
        "lr_g = 1e-4\n",
        "lr_d = 1e-5\n",
        "lambda_l1 = 100\n",
        "\n",
        "in_channels = 1\n",
        "out_channels = 3\n",
        "n_features_zero = 32\n",
        "group_norm = 8\n",
        "n_resblocks = 1\n",
        "num_dilated_convs = 4\n",
        "depth = 4\n",
        "kernel_size = 3\n",
        "padding = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiSZsLgLm-zx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_hidden_features = [n_features_zero * 2**k for k in range(depth)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOkO1YEQb9vR",
        "colab_type": "text"
      },
      "source": [
        "Define the generator and critic networks with the given hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK6LKVNe9mxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = UNet(in_channels, \n",
        "                 out_channels, \n",
        "                 num_hidden_features,\n",
        "                 n_resblocks,\n",
        "                 num_dilated_convs,\n",
        "                 dropout_min = 0.1, \n",
        "                 dropout_max = 0.3, \n",
        "                 gated = False, \n",
        "                 padding = 1, \n",
        "                 kernel_size = 3,\n",
        "                 group_norm = group_norm)\n",
        "\n",
        "generator = nn.Sequential(generator, nn.Tanh())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjbiANHY9ryk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator = Discriminator({'n_features_input':in_channels + out_channels,\n",
        "                              'num_hidden_features':num_hidden_features,\n",
        "                              'kernel_size':kernel_size,\n",
        "                              'padding':padding,\n",
        "                              'n_resblocks':n_resblocks,\n",
        "                              'dropout_min':0.1,\n",
        "                              'dropout_max':0.3, \n",
        "                              'blockObject':ResidualBlock,\n",
        "                              'batchNormObject':Identity})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvyKvnE7cFg6",
        "colab_type": "text"
      },
      "source": [
        "Initialize the weights (Xavier for weights and 0 for biases):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s38WTYV_nItl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d): \n",
        "        if m.weight is not None:\n",
        "            init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            init.constant_(m.bias, 0.0)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        if m.weight is not None:\n",
        "            init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            init.constant_(m.bias, 0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z2mPJm_dLZv",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "\n",
        "#### WGAN-GP Gradients Penlaty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB712FrTdLZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_gradient_penalty(netD, real_data, fake_data):\n",
        "    lmbd = 10 \n",
        "    alpha = torch.rand(real_data.size(0), 1).requires_grad_()\n",
        "    alpha = alpha[:,:,None,None]\n",
        "    alpha = alpha.expand(real_data.size())\n",
        "\n",
        "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
        "\n",
        "    disc_interpolates = netD(interpolates)\n",
        "\n",
        "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
        "                              grad_outputs=torch.ones(disc_interpolates.size()),\n",
        "                              create_graph = True, retain_graph=True, only_inputs=True)[0]\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lmbd\n",
        "    return gradient_penalty"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCrIcrmjdLZ5",
        "colab_type": "text"
      },
      "source": [
        "### Train Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLCj-6-CdLZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_discriminator(optimizer, real_data, real_labels, fake_data, fake_labels):\n",
        "    optimizer.zero_grad()\n",
        "    # rain on real data\n",
        "    real_input = torch.cat([real_data,real_labels],1)\n",
        "    prediction_real = discriminator(real_input).squeeze()\n",
        "    # train on take Data\n",
        "    fake_input = torch.cat([fake_data,fake_labels],1)\n",
        "    prediction_fake = discriminator(fake_input).squeeze()\n",
        "    # gradients penalty (Lipschitz condition) (WGAN-GP)\n",
        "    penalty = calc_gradient_penalty(discriminator,real_input, fake_input)\n",
        "    loss = prediction_fake.mean() - prediction_real.mean() + penalty\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7DSOKacdLaB",
        "colab_type": "text"
      },
      "source": [
        "### Train Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgAXDd8gdLaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_generator(optimizer, fake_data, fake_labels, true_data):\n",
        "    optimizer.zero_grad()\n",
        "    fake_input = torch.cat([fake_data,fake_labels],1)\n",
        "    prediction = discriminator(fake_input).squeeze() \n",
        "    G_loss = - torch.mean(prediction)\n",
        "    L1 = torch.abs(fake_data - true_data).mean()\n",
        "    loss = G_loss + lambda_l1 * L1\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return G_loss,L1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al3awW4HdLaJ",
        "colab_type": "text"
      },
      "source": [
        "## Define the training:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO1qm5AldDum",
        "colab_type": "text"
      },
      "source": [
        "Setting parameters for the training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig8Hujo0-pHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 20\n",
        "starting_epoch = 0\n",
        "g_error = 0\n",
        "gen_steps = 1\n",
        "gen_train_freq = 1\n",
        "global_step = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoR-3J-PnP6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "data_loader2 = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwE2Cv_xnNBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_optimizer = optim.Adam(discriminator.parameters(), lr = lr_d)\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr = lr_g)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSKnvdEddjzN",
        "colab_type": "text"
      },
      "source": [
        "Initializing the weights (or loading them from a checkpoint):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkQhsIe79xk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator.apply(weights_init)\n",
        "discriminator.apply(weights_init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIOpMyWQKL77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator.load_state_dict(torch.load('drive/My Drive/Deoldify/Model_3/Checkpoints/gen_epoch_7,gloss=-1.805633.pth'))\n",
        "discriminator.load_state_dict(torch.load('drive/My Drive/Deoldify/Model_3/Checkpoints/disc_epoch_7,gloss=0.793554.pth'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLKdj1fSds4j",
        "colab_type": "text"
      },
      "source": [
        "Training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEQvOig5dLaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_step = 0\n",
        "for epoch in range(starting_epoch, epochs):\n",
        "  for n_batch, (real_data,label_batch) in enumerate(data_loader):\n",
        "      with torch.no_grad():\n",
        "          fake_data = generator(label_batch)\n",
        "      d_error = train_discriminator(d_optimizer, real_data,label_batch, fake_data,label_batch)\n",
        "      if global_step % gen_train_freq == 0:\n",
        "          for _ in range(gen_steps):\n",
        "              real_data, label_batch = next(iter(data_loader2))\n",
        "              fake_data = generator(label_batch)\n",
        "              g_error,l1_error = train_generator(g_optimizer,fake_data,label_batch,real_data)\n",
        "              g_error,l1_error = g_error.item(),l1_error.item()\n",
        "      print('step ', global_step, ' epoch ', epoch, ' d_error ', d_error.item(), ' g_error ', g_error)\n",
        "      global_step += 1\n",
        "      if global_step % 5 == 0:\n",
        "          test_images = fake_data.permute(0,2,3,1).data.cpu().numpy()\n",
        "          i_ = 0\n",
        "          plt.figure(figsize=(5*m_test, 5*m_test)) \n",
        "          plt.subplots_adjust(wspace=0, hspace=0)\n",
        "          for l in range(m_test**2):\n",
        "              tile = test_images[l]\n",
        "              tile = (tile-tile.min())/(tile.max()-tile.min())\n",
        "              plt.subplot(m_test, m_test, i_+1) \n",
        "              plt.imshow(tile); plt.axis('off')\n",
        "              i_ += 1\n",
        "          plt.savefig('drive/My Drive/Deoldify/Model_3/Train/epoch' + str(epoch) + 'step' + str(global_step) + '.png', bbox_inches='tight')\n",
        "          #plt.show()\n",
        "      if global_step % 50 == 0:\n",
        "        torch.save(generator.state_dict(),\n",
        "                  '%s/gen_epoch_%d,gloss=%.6f.pth' % (\n",
        "                  'drive/My Drive/Deoldify/Model_3/Checkpoints/', epoch, g_error))\n",
        "        torch.save(discriminator.state_dict(),\n",
        "                  '%s/disc_epoch_%d,gloss=%.6f.pth' % (\n",
        "                  'drive/My Drive/Deoldify/Model_3/Checkpoints/', epoch, d_error))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpV0j_AdTt3I",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NZ61V_ofDRg",
        "colab_type": "text"
      },
      "source": [
        "Setting the path for the test images folder and loading the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SBKV62uTwhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_root = 'val2017/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDOxEw49UEON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = Load_Dataset(test_root,128)\n",
        "test_dataset[0][0].shape,dataset[0][1].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ck1cqajfKs9",
        "colab_type": "text"
      },
      "source": [
        "Batch-size for testing (default set to 1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXgJzI-TUbLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_batch_size = 1\n",
        "m_test = int(np.sqrt(test_batch_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEquhGYwUzui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size = test_batch_size, shuffle=True)\n",
        "test_data_loader2 = torch.utils.data.DataLoader(test_dataset, batch_size = test_batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJtuXJWHfSk8",
        "colab_type": "text"
      },
      "source": [
        "Loading the weights from the checkpoints:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwQd87lkeubA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator.load_state_dict(torch.load('drive/My Drive/Deoldify/Model_3/Checkpoints/gen_epoch_6,gloss=0.751362.pth'))\n",
        "discriminator.load_state_dict(torch.load('drive/My Drive/Deoldify/Model_3/Checkpoints/disc_epoch_6,gloss=0.436743.pth'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV7481EdfrnV",
        "colab_type": "text"
      },
      "source": [
        "Testing process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKhaDv_zVxa1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_test_step = 0\n",
        "for n_batch, (real_data,label_batch) in enumerate(test_data_loader):\n",
        "    with torch.no_grad():\n",
        "        fake_data = generator(label_batch)\n",
        "    if global_test_step % gen_train_freq == 0:\n",
        "        for _ in range(gen_steps):\n",
        "            real_data, label_batch = next(iter(test_data_loader2))\n",
        "            fake_data = generator(label_batch)\n",
        "    global_test_step += 1\n",
        "    if global_test_step % 1 == 0:\n",
        "        test_images = fake_data.permute(0,2,3,1).data.cpu().numpy()\n",
        "        real_images = real_data.permute(0,2,3,1).data.cpu().numpy()\n",
        "        i_ = 0\n",
        "        plt.figure(figsize=(5*m_test, 5*m_test)) \n",
        "        plt.subplots_adjust(wspace=0, hspace=0)\n",
        "        for l in range(m_test**2):\n",
        "            tile = test_images[l]\n",
        "            tile = (tile-tile.min())/(tile.max()-tile.min())\n",
        "            plt.subplot(m_test, m_test, i_+1) \n",
        "            plt.imshow(tile); plt.axis('off')\n",
        "            i_ += 1\n",
        "        plt.savefig('drive/My Drive/Deoldify/Model_3/Test/colorized_step' + str(global_test_step) + '.png', bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        i_ = 0\n",
        "        plt.figure(figsize=(5*m_test, 5*m_test)) \n",
        "        plt.subplots_adjust(wspace=0, hspace=0)\n",
        "        for l in range(m_test**2):\n",
        "            tile = real_images[l]\n",
        "            tile = (tile-tile.min())/(tile.max()-tile.min())\n",
        "            plt.subplot(m_test, m_test, i_+1) \n",
        "            plt.imshow(tile); plt.axis('off')\n",
        "            i_ += 1\n",
        "        plt.savefig('drive/My Drive/Deoldify/Model_3/Test/original_step' + str(global_test_step) + '.png', bbox_inches='tight')\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az2kND6b_nF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}